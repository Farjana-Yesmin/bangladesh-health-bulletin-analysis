# -*- coding: utf-8 -*-
"""Health_Bulletin_2019 and 2023.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-q8moBQm-HCGXN1V9QGIC0n1qElew0z3
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("farjanayesmin/bangladesh-health-bulletin-2019-dataset")

print("Path to dataset files:", path)

import os

# Check the KaggleHub extraction directory
dataset_dir = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/"
print("Files in the directory:")
print(os.listdir(dataset_dir))

import os

base_dir = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/"
for root, dirs, files in os.walk(base_dir):
    print(f"Directory: {root}")
    for file in files:
        print(f" - {file}")

# Corrected file paths
base_path = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/versions/1/"

# Load the CSV files
demographics_data = pd.read_csv(base_path + "Demographics_Health_Bulletin_2019.csv")
disease_data = pd.read_csv(base_path + "Disease_Statistics_Health_Bulletin_2019.csv")
healthcare_data = pd.read_csv(base_path + "Healthcare_Infrastructure_Health_Bulletin_2019.csv")
policy_data = pd.read_csv(base_path + "Healthcare_Financing_and_Policy_Health_Bulletin_2019.csv")
services_data = pd.read_csv(base_path + "Health_Services_Utilization_Health_Bulletin_2019.csv")
workforce_data = pd.read_csv(base_path + "Health_Workforce_Health_Bulletin_2019.csv")
education_data = pd.read_csv(base_path + "Health_Education_and_Training_Health_Bulletin_2019.csv")

# Verify by displaying the first few rows of one dataset
print(demographics_data.head())

# Display the first few rows
print(demographics_data.head())

# Check for unique values to understand the data
print(demographics_data['Extracted Information'].unique())

# Filter rows containing keywords for measurable indicators
keywords = ['mortality', 'population', 'growth', 'rate', 'fertility', 'density', 'literacy']
filtered_data = demographics_data[demographics_data['Extracted Information']
                                  .str.contains('|'.join(keywords), case=False, na=False)]

print(filtered_data)

# Extract rows with a pattern like "Indicator: Value"
indicators = filtered_data['Extracted Information'].str.extract(r'(?P<Indicator>.*?): (?P<Value>[\d\.]+)')
indicators.dropna(inplace=True)  # Remove rows where extraction failed
print(indicators.head())

mortality_data = indicators[indicators['Indicator'].str.contains('mortality', case=False)]
population_data = indicators[indicators['Indicator'].str.contains('population', case=False)]
literacy_data = indicators[indicators['Indicator'].str.contains('literacy', case=False)]

indicators['Value'] = pd.to_numeric(indicators['Value'], errors='coerce')
indicators.dropna(subset=['Value'], inplace=True)

print(mortality_data.columns)

# Check the contents of the mortality_data DataFrame
print(mortality_data.head())

# Inspect the column names and data types
print(mortality_data.info())

import pandas as pd

# Define the base path for the dataset
base_path = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/versions/1/"

# Load the CSV files into separate DataFrames
demographics_data = pd.read_csv(base_path + "Demographics_Health_Bulletin_2019.csv")
disease_data = pd.read_csv(base_path + "Disease_Statistics_Health_Bulletin_2019.csv")
healthcare_data = pd.read_csv(base_path + "Healthcare_Infrastructure_Health_Bulletin_2019.csv")
policy_data = pd.read_csv(base_path + "Healthcare_Financing_and_Policy_Health_Bulletin_2019.csv")
services_data = pd.read_csv(base_path + "Health_Services_Utilization_Health_Bulletin_2019.csv")
workforce_data = pd.read_csv(base_path + "Health_Workforce_Health_Bulletin_2019.csv")
education_data = pd.read_csv(base_path + "Health_Education_and_Training_Health_Bulletin_2019.csv")

# Verify by displaying the first few rows of each dataset
print("Demographics Data:")
print(demographics_data.head(), "\n")

print("Disease Statistics Data:")
print(disease_data.head(), "\n")

print("Healthcare Infrastructure Data:")
print(healthcare_data.head(), "\n")

print("Healthcare Financing and Policy Data:")
print(policy_data.head(), "\n")

print("Health Services Utilization Data:")
print(services_data.head(), "\n")

print("Health Workforce Data:")
print(workforce_data.head(), "\n")

print("Health Education and Training Data:")
print(education_data.head(), "\n")

print(demographics_data.columns)
print(disease_data.columns)

import re

demographics_data['Year'] = demographics_data['Extracted Information'].str.extract(r'(\d{4})')
print(demographics_data[['Extracted Information', 'Year']].head())

def categorize_information(text):
    text = text.lower()  # Normalize text for easier matching
    if "mortality" in text:
        return "Mortality"
    elif "health" in text:
        return "Health"
    elif "infrastructure" in text:
        return "Infrastructure"
    elif "education" in text:
        return "Education"
    elif "policy" in text:
        return "Policy"
    elif "workforce" in text:
        return "Workforce"
    else:
        return "Other"

demographics_data['Category'] = demographics_data['Extracted Information'].apply(categorize_information)
print(demographics_data[['Extracted Information', 'Category']].head(10))

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each category
category_counts = demographics_data['Category'].value_counts()

# Plot the distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=category_counts.index, y=category_counts.values, palette="viridis")
plt.title('Distribution of Categories in Demographics Data')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""# Filter the Data for "Mortality"
"""

# Filter the data for "Mortality" category
mortality_data = demographics_data[demographics_data['Category'] == 'Mortality']

# Display the first few rows
print(mortality_data.head())

"""# Analyze Missing Data"""

# Check the unique values in the "Extracted Information" column for "Mortality" entries
print(mortality_data['Extracted Information'].unique())

"""# Extract Numeric Values"""

# Refined regular expression to extract all numbers, including decimal and comma separated
mortality_data.loc[:, 'Value'] = mortality_data['Extracted Information'].str.extract(r'(\d+(?:[\.,]\d+)*)')

# Convert 'Value' column to numeric, forcing errors to NaN
mortality_data.loc[:, 'Value'] = pd.to_numeric(mortality_data['Value'], errors='coerce')

# Display the first few rows of the updated data
print(mortality_data[['Extracted Information', 'Value']].head())

"""# Visual Inspection"""

# Show rows with 'Mortality' category and 'Extracted Information' to check patterns
print(mortality_data[mortality_data['Category'] == 'Mortality']['Extracted Information'].head(20))

"""# New Regular Expression"""

# Use .loc to avoid SettingWithCopyWarning and ensure safe assignment
mortality_data.loc[:, 'Value'] = mortality_data['Extracted Information'].str.extract(r'(\d+(?:[,.]\d{1,2})?|\d{1,3}(?:,\d{3})*)')

# Convert the extracted values to numeric, coercing errors to NaN
mortality_data.loc[:, 'Value'] = pd.to_numeric(mortality_data['Value'], errors='coerce')

# Display the result for inspection
print(mortality_data[['Extracted Information', 'Value']].head(20))

# Check the extracted text from row 31
print(mortality_data.loc[31, 'Extracted Information'])

print(mortality_data['Extracted Information'][mortality_data['Value'].isna()].head(20))

r'(\d{1,3}(?:[\.,]?\d{3})*(?:[\.,]?\d+)?(?:\s*per\s*\d{1,3}[\w\s]*)?)'

# Checking rows with keywords for 'under-five mortality rate' and similar terms
mortality_data['Value'] = mortality_data['Extracted Information'].str.extract(r'(\d+(?:[\.,]?\d+)?(?:\s*per\s*\d{1,3}[\w\s]*)?)')

# Additional check for specific keywords like 'under-five' or 'infant'
keyword_rows = mortality_data[mortality_data['Extracted Information'].str.contains(r'under-five|infant|neonatal', case=False, na=False)]
print(keyword_rows[['Extracted Information', 'Value']])

mortality_data['Value'] = mortality_data['Extracted Information'].str.extract(r'(\d{1,3}(?:[\.,]?\d{3})*(?:[\.,]?\d+)?(?:\s*per\s*\d{1,3}[\w\s]*)?)')

cleaned_data = mortality_data.dropna(subset=['Value'])
print(cleaned_data[['Extracted Information', 'Value']])

cleaned_data = mortality_data.dropna(subset=['Value'])
print(cleaned_data[['Extracted Information', 'Value']].head(10))

"""# Visualization of Mortality Rates"""

import matplotlib.pyplot as plt
import seaborn as sns

# Cleaned data with relevant mortality values
cleaned_data = mortality_data.dropna(subset=['Value'])

# Focus on relevant columns for plotting
mortality_types = cleaned_data[['Extracted Information', 'Value']]

# Convert 'Value' to numeric (in case it's in string format with commas or 'per' text)
mortality_types['Value'] = mortality_types['Value'].apply(pd.to_numeric, errors='coerce')

# Plot the mortality rates
plt.figure(figsize=(10, 6))
sns.barplot(data=mortality_types, x='Extracted Information', y='Value', palette='viridis')
plt.xticks(rotation=90)
plt.xlabel('Type of Mortality')
plt.ylabel('Rate Value')
plt.title('Comparison of Mortality Rates')
plt.tight_layout()
plt.show()

"""# Refining the Data Extraction"""

# Define a list of relevant keywords
mortality_keywords = ['Infant', 'Neonatal', 'Under-5', 'Maternal', 'Malarial']

# Filter rows that contain any of these keywords in 'Extracted Information'
filtered_data = mortality_data[mortality_data['Extracted Information'].str.contains('|'.join(mortality_keywords), case=False, na=False)]

# Ensure that the operation is done in-place with .loc to avoid the SettingWithCopyWarning
filtered_data.loc[:, 'Value'] = filtered_data['Value'].apply(pd.to_numeric, errors='coerce')

# Drop rows where 'Value' is NaN
filtered_data = filtered_data.dropna(subset=['Value'])

# Preview the filtered data
print(filtered_data[['Extracted Information', 'Value']])

"""# Group Similar Mortality Types"""

# Step 1: Remove irrelevant rows
filtered_data_cleaned = filtered_data[~filtered_data['Extracted Information'].str.contains('Figure|status|declining', case=False, na=False)]

# Step 2: Group similar mortality types (e.g., all variations of "Under-5 mortality")
grouped_data = filtered_data_cleaned.groupby('Extracted Information')['Value'].mean().reset_index()

# Step 3: Preview the cleaned and grouped data
print(grouped_data[['Extracted Information', 'Value']])

"""# Remove redundant entries and finalize the grouping"""

# Step 1: Remove near-duplicate or redundant entries
cleaned_final_data = grouped_data[~grouped_data['Extracted Information'].str.contains('Under-5 mortality rate', case=False, na=False)]

# Step 2: Further refine the data to ensure proper grouping of similar mortality types
final_grouped_data = cleaned_final_data.groupby('Extracted Information')['Value'].mean().reset_index()

# Step 3: Preview the final cleaned and grouped data
print(final_grouped_data[['Extracted Information', 'Value']])

# Save the cleaned dataset as a CSV file
cleaned_data.to_csv('cleaned_mortality_data.csv', index=False)

import os

# Check if the file exists
file_path = '/content/cleaned_mortality_data.csv'

if os.path.exists(file_path):
    print(f"The file {file_path} exists.")
else:
    print(f"The file {file_path} does not exist.")

# Check if the Disease Statistics data is loaded correctly
print(disease_data.head())  # Preview the first few rows

# Check for missing values or duplicates in the Disease Statistics dataset
print(disease_data.isnull().sum())  # Check for missing values
print(disease_data.duplicated().sum())  # Check for duplicate rows

# Check the column names and first few rows of the disease data
print(disease_data.columns)
print(disease_data.head())

# Print out a few more rows of the disease data to inspect for possible numerical values
print(disease_data.head(20))

import re

# Define a function to extract numerical values from text
def extract_numerical_value(text):
    # Find all numbers (integers or decimals) in the text
    numbers = re.findall(r"[-+]?\d*\.\d+|\d+", str(text))
    if numbers:
        return float(numbers[0])  # Return the first number found
    return None  # Return None if no numbers are found

# Apply this function to extract numerical values
disease_data['Value'] = disease_data['Extracted Information'].apply(extract_numerical_value)

# Filter out rows where 'Value' is None (no numerical value found)
relevant_disease_data = disease_data.dropna(subset=['Value'])

# Display the first few rows of the cleaned data
print(relevant_disease_data.head())

# Now you can proceed with grouping or further cleaning if necessary

# Step 1: Group the data by 'Extracted Information' and calculate the mean of the 'Value' column (if applicable)
cleaned_disease_data = relevant_disease_data.groupby('Extracted Information')['Value'].mean().reset_index()

# Step 2: Save the cleaned disease data to a CSV file
cleaned_disease_data.to_csv('cleaned_disease_data.csv', index=False)

# Step 3: Preview the cleaned data
print(cleaned_disease_data.head())

# Step 1: Save the cleaned disease data to a CSV file
cleaned_disease_data.to_csv('/content/cleaned_disease_data.csv', index=False)

# Step 2: Preview the cleaned data
print(cleaned_disease_data.head())

# Corrected file path (assuming you have all CSV files loaded correctly in your environment)
base_path = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/versions/1/"

# Load the Healthcare Infrastructure CSV file
healthcare_infrastructure_data = pd.read_csv(base_path + "Healthcare_Infrastructure_Health_Bulletin_2019.csv")

# Display the first few rows of the data to understand its structure
print(healthcare_infrastructure_data.head())

# Step 1: Inspect the data structure
print(healthcare_infrastructure_data.info())
print(healthcare_infrastructure_data.head())

# Step 2: Clean the data (remove redundant entries, handle missing values, etc.)
# For example, we could remove rows with NaN values or irrelevant text.
cleaned_healthcare_infrastructure_data = healthcare_infrastructure_data.dropna()

# Step 3: Check for and handle duplicate entries if necessary
cleaned_healthcare_infrastructure_data = cleaned_healthcare_infrastructure_data.drop_duplicates()

# Step 4: Preview the cleaned data
print(cleaned_healthcare_infrastructure_data.head())

# Optionally, save the cleaned data
cleaned_healthcare_infrastructure_data.to_csv('cleaned_healthcare_infrastructure_data.csv', index=False)

# Step 1: Clean the data (remove redundant text, handle irrelevant content)
# Example: Remove rows that are too general or unrelated to healthcare infrastructure
# We can remove entries containing certain keywords that are not directly useful.
irrelevant_keywords = ['clinic', 'hospital', 'health center', 'satellite', 'tuberculosis']

# Filter rows that contain the relevant keywords
relevant_healthcare_data = healthcare_infrastructure_data[healthcare_infrastructure_data['Extracted Information'].str.contains('|'.join(irrelevant_keywords), case=False, na=False)]

# Step 2: Remove duplicates if necessary
cleaned_healthcare_infrastructure_data = relevant_healthcare_data.drop_duplicates()

# Step 3: Preview the cleaned data
print(cleaned_healthcare_infrastructure_data.head())

# Optionally, save the cleaned data
cleaned_healthcare_infrastructure_data.to_csv('cleaned_healthcare_infrastructure_data.csv', index=False)

# Save the cleaned data
cleaned_healthcare_infrastructure_data.to_csv('cleaned_healthcare_infrastructure_data.csv', index=False)

# Confirm the data is saved
print("Cleaned Healthcare Infrastructure Data saved.")

# Corrected file path
base_path = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/versions/1/"

# Load the Healthcare Financing and Policy CSV file
policy_data = pd.read_csv(base_path + "Healthcare_Financing_and_Policy_Health_Bulletin_2019.csv")

# Display the first few rows of the data to understand its structure
print(policy_data.head())

# Display the first few rows of the 'Healthcare Financing and Policy' data
print(policy_data.head())

# Check the column names to understand the structure of the dataset
print(policy_data.columns)

# Check for any missing or null values
print(policy_data.isnull().sum())

# Check the data types of each column
print(policy_data.dtypes)

# Step 1: Remove duplicates
cleaned_policy_data = policy_data.drop_duplicates(subset=['Extracted Information'])

# Step 2: Optionally, further clean the data (e.g., remove any unnecessary whitespace)
cleaned_policy_data.loc[:, 'Extracted Information'] = cleaned_policy_data['Extracted Information'].str.strip()

# Step 3: Display the cleaned data preview
print(cleaned_policy_data.head())

# Step 4: Save the cleaned data to a new CSV file
cleaned_policy_data.to_csv('/content/cleaned_healthcare_financing_and_policy.csv', index=False)

# Save the cleaned data to a new CSV file
cleaned_policy_data.to_csv('/content/cleaned_healthcare_financing_and_policy.csv', index=False)

import os

# List the files in the current directory to check if the file is saved
os.listdir('/content/')

# File path for Health Services Utilization data
file_path_services_data = base_path + "Health_Services_Utilization_Health_Bulletin_2019.csv"

# Load the data into a DataFrame
services_data = pd.read_csv(file_path_services_data)

# Display the first few rows of the data to understand its structure
print(services_data.head())

# Step 1: Strip any leading/trailing whitespace
services_data['Extracted Information'] = services_data['Extracted Information'].str.strip()

# Step 2: (Optional) Group or clean data further if needed (e.g., identifying specific topics)

# Save the cleaned data to a new CSV file
cleaned_services_data_path = '/content/cleaned_health_services_utilization.csv'
services_data.to_csv(cleaned_services_data_path, index=False)

print(f"Cleaned data saved to {cleaned_services_data_path}")

# Corrected file path (assuming the file is located correctly)
file_path_health_workforce = "/root/.cache/kagglehub/datasets/farjanayesmin/bangladesh-health-bulletin-2019-dataset/versions/1/Health_Workforce_Health_Bulletin_2019.csv"

# Load the data into a DataFrame
health_workforce_data = pd.read_csv(file_path_health_workforce)

# Display the first few rows of the data to understand its structure
print(health_workforce_data.head())

# Clean the 'Extracted Information' column by stripping any leading or trailing spaces
health_workforce_data['Extracted Information'] = health_workforce_data['Extracted Information'].str.strip()

# Display the first few rows to ensure cleaning is done correctly
print(health_workforce_data.head())

# Save the cleaned data to a new CSV file
health_workforce_data.to_csv('/content/cleaned_health_workforce_data.csv', index=False)

print("Cleaned data saved to /content/cleaned_health_workforce_data.csv")

# Corrected file path for the Health Education and Training data
file_path_education_data = base_path + "Health_Education_and_Training_Health_Bulletin_2019.csv"

# Load the data into a DataFrame
education_data = pd.read_csv(file_path_education_data)

# Display the first few rows of the data to understand its structure
print(education_data.head())

# Clean the data
education_data['Extracted Information'] = education_data['Extracted Information'].str.strip()

# Save the cleaned data to a new CSV file
education_data.to_csv('/content/cleaned_health_education_and_training_data.csv', index=False)

print("Cleaned data saved to /content/cleaned_health_education_and_training_data.csv")

"""# Data Summary & Descriptive Statistics for All Cleaned CSVs"""

# Load the cleaned Mortality data (assuming this is the Demographics data)
mortality_data = pd.read_csv('/content/cleaned_mortality_data.csv')

# Display a summary of the Mortality data
print("Mortality Data Summary:")
print(mortality_data.describe())

# Load the cleaned Disease data
disease_data = pd.read_csv('/content/cleaned_disease_data.csv')

# Display a summary of the Disease data
print("Disease Data Summary:")
print(disease_data.describe())

# Load the cleaned Healthcare Infrastructure data
healthcare_infrastructure_data = pd.read_csv('/content/cleaned_healthcare_infrastructure_data.csv')

# Display a summary of the Healthcare Infrastructure data
print("Healthcare Infrastructure Data Summary:")
print(healthcare_infrastructure_data.describe())

# Load the cleaned Health Workforce data
health_workforce_data = pd.read_csv('/content/cleaned_health_workforce_data.csv')

# Display a summary of the Health Workforce data
print("Health Workforce Data Summary:")
print(health_workforce_data.describe())

# Load the cleaned Health Education and Training data
health_education_data = pd.read_csv('/content/cleaned_health_education_and_training_data.csv')

# Display a summary of the Health Education and Training data
print("Health Education and Training Data Summary:")
print(health_education_data.describe())

"""# Correlation and Relationships Between Variables

# For Disease and Mortality Data
"""

# Load the cleaned Disease data
disease_data = pd.read_csv('/content/cleaned_disease_data.csv')

# Filter the dataset to keep only numerical columns
disease_numerical_data = disease_data.select_dtypes(include=['number'])

# Check if there are any numerical columns
if not disease_numerical_data.empty:
    # Calculate correlation matrix for Disease data
    disease_corr = disease_numerical_data.corr()

    # Plot the correlation matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(disease_corr, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Disease Data Correlation Matrix')
    plt.show()
else:
    print("No numerical data available for correlation analysis in the Disease data.")

# Load the cleaned Mortality data
mortality_data = pd.read_csv('/content/cleaned_mortality_data.csv')

# Filter the dataset to keep only numerical columns
mortality_numerical_data = mortality_data.select_dtypes(include=['number'])

# Check if there are any numerical columns
if not mortality_numerical_data.empty:
    # Calculate correlation matrix for Mortality data
    mortality_corr = mortality_numerical_data.corr()

    # Plot the correlation matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mortality_corr, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Mortality Data Correlation Matrix')
    plt.show()
else:
    print("No numerical data available for correlation analysis in the Mortality data.")

import pandas as pd

# Load the cleaned Healthcare Financing and Policy data
healthcare_financing_data = pd.read_csv('/content/cleaned_healthcare_financing_and_policy.csv')

# Display a summary of the data
print("Healthcare Financing and Policy Data Summary:")
print(healthcare_financing_data.describe(include='all'))

# Check for missing values in the dataset
print("\nMissing Values:")
print(healthcare_financing_data.isnull().sum())

# Check the data types of the columns
print("\nData Types:")
print(healthcare_financing_data.dtypes)

# Display the first few rows of the dataset to understand its structure
print("\nFirst Few Rows of Healthcare Financing and Policy Data:")
print(healthcare_financing_data.head())

# Optional: If there are any further transformations or cleaning required, you can proceed.

"""# Healthcare Infrastructure Data"""

import matplotlib.pyplot as plt

# Count the frequency of unique values in the 'Extracted Information' column
value_counts = healthcare_infrastructure_data['Extracted Information'].value_counts()

# Display the top 10 most common values
print(value_counts.head(10))

# Plot the frequency of the top 10 most common categories
plt.figure(figsize=(12, 8))
value_counts.head(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Most Common Categories in Healthcare Infrastructure Data')
plt.xlabel('Categories')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

"""# Health Workforce Data"""

# Load the cleaned Health Workforce data
health_workforce_data = pd.read_csv('/content/cleaned_health_workforce_data.csv')

# Display a summary of the Health Workforce data
print("Health Workforce Data Summary:")
print(health_workforce_data.describe())
print("\nMissing Values:")
print(health_workforce_data.isnull().sum())
print("\nData Types:")
print(health_workforce_data.dtypes)

# Check the first few rows of the data to inspect the structure
print("\nFirst Few Rows of Health Workforce Data:")
print(health_workforce_data.head())

# We are proceeding to analyze correlations. As the data is text-based, we might need to check for numeric columns or transformations.
# If there are numeric columns or data that can be quantified, we can calculate correlations here.

# If no numerical columns are found, we can examine the frequency or relationships of categorical variables.

"""# Extract Numerical Data from Text (e.g., Number of Doctors, Medical Technologists)"""

import re

# Define a function to extract numbers from the text
def extract_numbers(text):
    numbers = re.findall(r'\d+', text)  # Find all numbers
    return [int(num) for num in numbers]

# Apply the extraction function to the 'Extracted Information' column
health_workforce_data['extracted_numbers'] = health_workforce_data['Extracted Information'].apply(lambda x: extract_numbers(x))

# Now, let's inspect the extracted numbers
print("Extracted Numbers from Health Workforce Data:")
print(health_workforce_data[['Extracted Information', 'extracted_numbers']].head())

# You can also summarize the total numbers extracted, for example, total number of doctors, etc.

"""# Health Education and Training Data"""

# Load the cleaned Health Education and Training data
health_education_data = pd.read_csv('/content/cleaned_health_education_and_training_data.csv')

# Display the first few rows of the data to understand its structure
print("First few rows of Health Education and Training Data:")
print(health_education_data.head())

# If there are any numerical variables, we can calculate the correlation matrix
# Check for numerical columns
numerical_columns = health_education_data.select_dtypes(include=['number']).columns
if len(numerical_columns) > 0:
    correlation_matrix = health_education_data[numerical_columns].corr()

    # Plot the correlation matrix
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
    plt.title("Health Education and Training Data Correlation Matrix")
    plt.show()
else:
    print("No numerical columns found in Health Education and Training data.")

# Let's inspect extracted numeric data from 'extracted_numbers' column
print("Extracted Numbers from Health Workforce Data:")
print(health_workforce_data[['Extracted Information', 'extracted_numbers']].head())

# Now, let's check the frequency of extracted numbers (e.g., counts of doctors, etc.)
# Flatten the list of numbers to count frequencies of individual values
flattened_numbers = [num for sublist in health_workforce_data['extracted_numbers'] for num in sublist]

# Create a frequency distribution of the numbers
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 8))
sns.histplot(flattened_numbers, kde=False, color='skyblue', bins=20)
plt.title("Frequency Distribution of Extracted Numbers from Health Workforce Data")
plt.xlabel("Extracted Numbers")
plt.ylabel("Frequency")
plt.show()

# Load the cleaned Health Education and Training data
health_education_data = pd.read_csv('/content/cleaned_health_education_and_training_data.csv')

# Count the frequency of unique values in the 'Extracted Information' column
value_counts_education = health_education_data['Extracted Information'].value_counts()

# Display the top 10 most common values
print(value_counts_education.head(10))

# Plot the frequency of the top 10 most common categories
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
value_counts_education.head(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Most Common Categories in Health Education and Training Data')
plt.xlabel('Categories')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

import os

# List all files in the /content directory to verify the cleaned CSV file names
os.listdir('/content')

import zipfile

# List of cleaned CSV files to be included in the ZIP file
cleaned_csv_files = [
    '/content/cleaned_health_education_and_training_data.csv',
    '/content/cleaned_disease_data.csv',
    '/content/cleaned_healthcare_financing_and_policy_data.csv',
    '/content/cleaned_healthcare_infrastructure_data.csv',
    '/content/cleaned_health_workforce_data.csv',
    '/content/cleaned_health_services_utilization.csv',
    '/content/cleaned_healthcare_financing_and_policy.csv',
    '/content/cleaned_mortality_data.csv'
]

# Path for the ZIP file
zip_file_path = '/content/cleaned_health_data.zip'

# Create the ZIP file and add all the cleaned CSV files
with zipfile.ZipFile(zip_file_path, 'w') as zipf:
    for file in cleaned_csv_files:
        zipf.write(file, arcname=file.split('/')[-1])  # Add the file to the zip archive with its name

print(f"ZIP file created at {zip_file_path}. You can download it now.")

import requests

url = "https://dghs.portal.gov.bd/sites/default/files/files/dghs.portal.gov.bd/page/8983ee81_3668_4bc3_887e_c99645bbfce4/2024-11-20-13-44-b1d4c5ef4a96a9eda93ad802ce1ddaa8.pdf"
response = requests.get(url, verify=False)
with open("Health_Bulletin_2023.pdf", "wb") as f:
    f.write(response.content)
print("PDF downloaded.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install pdfplumber

import pdfplumber
import pandas as pd
import re  # For cleaning text

# Function to extract text/tables from specific pages
def extract_from_pdf(pdf_path, page_ranges):
    all_data = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_num in page_ranges:
            page = pdf.pages[page_num - 1]  # 0-indexed
            text = page.extract_text()
            tables = page.extract_tables()
            all_data.append({
                'page': page_num,
                'text': text,
                'tables': tables
            })
    return all_data

# Example: Extract from key sections (adjust based on actual pages)
extracted_data = extract_from_pdf("Health_Bulletin_2023.pdf", range(20, 351))  # Full range; narrow for efficiency

# Save raw extracted text to a file for review
with open("extracted_2023_text.txt", "w") as f:
    for item in extracted_data:
        f.write(f"Page {item['page']}:\n{item['text']}\n\n")
print("Raw text extracted.")

# Categories similar to your 2019 CSVs
categories = {
    "demographics": ["population", "mortality", "birth rate", "literacy", "dependency ratio"],
    "workforce": ["doctors", "nurses", "technologists", "posts", "vacancies", "ratio"],
    "diseases": ["malaria", "tuberculosis", "HIV", "dengue", "kala-azar", "NCD", "communicable"],
    "infrastructure": ["hospitals", "beds", "clinics", "facilities", "community clinics"],
    "services": ["immunization", "antenatal", "delivery", "PNC", "utilization"],
    "financing_policy": ["financing", "policy", "program", "SDG", "HPNSP", "funding"],
    "education_training": ["education", "training", "medical colleges", "institutions", "enrolment", "courses"]
}

# Function to parse text into DataFrame and save CSV
def parse_and_save(category, extracted_data):
    data_list = []
    for item in extracted_data:
        text_lower = item['text'].lower() if item['text'] else ""
        if any(keyword in text_lower for keyword in categories[category]):
            # Extract key-value pairs (e.g., "Population: 164.6" -> {'key': 'Population', 'value': '164.6'})
            matches = re.findall(r"(\w+[\w\s]*?):\s*([\d\.\%]+)", item['text'], re.IGNORECASE | re.MULTILINE)
            for match in matches:
                data_list.append({'key': match[0].strip(), 'value': match[1].strip(), 'page': item['page']})

            # If tables found, flatten them
            for table in item['tables']:
                df_table = pd.DataFrame(table[1:], columns=table[0])  # Assume first row is header
                data_list.extend(df_table.to_dict(orient='records'))

    if data_list:
        df = pd.DataFrame(data_list)
        csv_path = f"raw_{category}_2023.csv"
        df.to_csv(csv_path, index=False)
        print(f"Saved {csv_path}")
    return df if data_list else None

# Run for all categories
for cat in categories:
    parse_and_save(cat, extracted_data)

# Example: Load and clean a CSV (repeat for each category)
def clean_csv(input_path, output_path):
    df = pd.read_csv(input_path)
    df = df.dropna(subset=['key', 'value'])  # Drop missing key-value
    df['value'] = pd.to_numeric(df['value'], errors='coerce')  # Convert values to numeric
    df = df.dropna(subset=['value'])  # Drop non-numeric after conversion
    df.to_csv(output_path, index=False)
    print(f"Cleaned and saved {output_path}")

# Clean all
for cat in categories:
    clean_csv(f"raw_{cat}_2023.csv", f"cleaned_{cat}_2023.csv")

import zipfile

cleaned_files = [f"cleaned_{cat}_2023.csv" for cat in categories]
zip_path = "cleaned_health_bulletin_2023.zip"
with zipfile.ZipFile(zip_path, 'w') as zipf:
    for file in cleaned_files:
        zipf.write(file)
print("Zipped cleaned CSVs.")

# Load example CSVs
demographics = pd.read_csv("cleaned_demographics_2023.csv")
workforce = pd.read_csv("cleaned_workforce_2023.csv")

# Print column names to debug
print("Columns in workforce DataFrame:", workforce.columns.tolist())
print("Columns in demographics DataFrame:", demographics.columns.tolist())

# Visualizations
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Distribution (e.g., population metrics)
sns.histplot(demographics['value'], kde=True)
plt.title("Distribution of Demographic Values (2023)")
plt.show()

# Correlation (e.g., between workforce count and vacancies)
# Adjust column names based on debug output
if 'Workforce_Count' in workforce.columns and 'Vacant_Posts' in workforce.columns:
    sns.heatmap(workforce[['Workforce_Count', 'Vacant_Posts']].corr(), annot=True)
    plt.title("Workforce Correlations (2023)")
    plt.show()
else:
    print("Correlation plot skipped: 'Workforce_Count' or 'Vacant_Posts' not found. Available columns:", workforce.columns.tolist())

# Regression example (adapt with correct column names)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score

# Adjust column names based on debug output
print("Checking for regression columns...")
if 'Staff_Category' in workforce.columns and 'Workforce_Total' in workforce.columns:  # Example names; adjust
    encoder = LabelEncoder()
    workforce['Staff_Category'] = encoder.fit_transform(workforce['Staff_Category'].fillna('Unknown'))
    X = workforce[['Staff_Category']]
    y = workforce['Workforce_Total']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"MAE: {mae}, R2: {r2}")
else:
    print("Regression skipped: 'Staff_Category' or 'Workforce_Total' not found. Available columns:", workforce.columns.tolist())

import matplotlib.pyplot as plt
import pandas as pd

# Manual data entry for 2019 and 2023 crude death rates
years = ['2019', '2023']
rates = [5.26, 5.01]  # per 1,000 population

plt.figure(figsize=(6, 4))
plt.bar(years, rates)
plt.title("Crude Death Rate Comparison (2019 vs. 2023)")
plt.xlabel("Year")
plt.ylabel("Crude Death Rate (per 1,000)")
plt.tight_layout()
plt.savefig("mortality_comparison.png", dpi=300, bbox_inches='tight')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Sample data from bulletins (extracted via your notebook)
data = {
    'Year': [2019, 2023],
    'Crude Death Rate': [5.26, 6.1]  # From 2019 bulletin and 2023 PDF
}

df = pd.DataFrame(data)
plt.figure(figsize=(8, 5))
plt.plot(df['Year'], df['Crude Death Rate'], marker='o', color='green', linewidth=2)
plt.title('Crude Death Rate Trends (2019-2023)', fontsize=14)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Rate per 1,000 Population', fontsize=12)
plt.grid(True, linestyle='--')
plt.xticks([2019, 2023])
plt.savefig('crude_death_rate.png', dpi=300)  # High-quality save
plt.show()